{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "australian-hamburg",
   "metadata": {
    "papermill": {
     "duration": 0.017983,
     "end_time": "2021-06-15T14:06:07.535350",
     "exception": false,
     "start_time": "2021-06-15T14:06:07.517367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Spam Email Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "athletic-bunch",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-15T14:06:07.575863Z",
     "iopub.status.busy": "2021-06-15T14:06:07.574084Z",
     "iopub.status.idle": "2021-06-15T14:06:09.462357Z",
     "shell.execute_reply": "2021-06-15T14:06:09.461490Z",
     "shell.execute_reply.started": "2021-06-15T12:59:13.607513Z"
    },
    "papermill": {
     "duration": 1.910122,
     "end_time": "2021-06-15T14:06:09.462542",
     "exception": false,
     "start_time": "2021-06-15T14:06:07.552420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing system libraries\n",
    "from os import walk\n",
    "from pathlib import Path\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "from collections import Counter\n",
    "\n",
    "# importing additional libraries\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "perfect-examination",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T14:06:09.505042Z",
     "iopub.status.busy": "2021-06-15T14:06:09.504262Z",
     "iopub.status.idle": "2021-06-15T14:09:26.583467Z",
     "shell.execute_reply": "2021-06-15T14:09:26.582550Z",
     "shell.execute_reply.started": "2021-06-15T12:59:18.262459Z"
    },
    "papermill": {
     "duration": 197.103368,
     "end_time": "2021-06-15T14:09:26.583671",
     "exception": false,
     "start_time": "2021-06-15T14:06:09.480303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the whole data from the Enron Dataset into a variable allData.\n",
    "rawpath = r\"/kaggle/input/enron-spam/\"\n",
    "path = Path(rawpath)\n",
    "pathwalk = walk(path)\n",
    "\n",
    "allHamData, allSpamData = [], []\n",
    "for root, dr, file in pathwalk:\n",
    "    if 'ham' in str(file):\n",
    "        for obj in file:\n",
    "            with open(root + '/' + obj, encoding='latin1') as ip:\n",
    "                allHamData.append(\" \".join(ip.readlines()))\n",
    "                \n",
    "    elif 'spam' in str(file):\n",
    "        for obj in file:\n",
    "            with open(root + '/' + obj, encoding='latin1') as ip:\n",
    "                allSpamData.append(\" \".join(ip.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "black-project",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T14:09:26.671662Z",
     "iopub.status.busy": "2021-06-15T14:09:26.670674Z",
     "iopub.status.idle": "2021-06-15T14:09:26.673266Z",
     "shell.execute_reply": "2021-06-15T14:09:26.673720Z",
     "shell.execute_reply.started": "2021-06-15T13:00:56.362361Z"
    },
    "papermill": {
     "duration": 0.070695,
     "end_time": "2021-06-15T14:09:26.673910",
     "exception": false,
     "start_time": "2021-06-15T14:09:26.603215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove all redundent data\n",
    "allHamData = list(set(allHamData))\n",
    "allSpamData = list(set(allSpamData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sporting-hypothetical",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T14:09:26.714596Z",
     "iopub.status.busy": "2021-06-15T14:09:26.713657Z",
     "iopub.status.idle": "2021-06-15T14:09:26.720568Z",
     "shell.execute_reply": "2021-06-15T14:09:26.719508Z",
     "shell.execute_reply.started": "2021-06-15T13:03:50.511737Z"
    },
    "papermill": {
     "duration": 0.029761,
     "end_time": "2021-06-15T14:09:26.720900",
     "exception": false,
     "start_time": "2021-06-15T14:09:26.691139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of ham emails: 15910\n",
      "number of spam emails: 14583\n"
     ]
    }
   ],
   "source": [
    "# get an overview of the data\n",
    "print(\"number of ham emails:\", len(allHamData))\n",
    "print(\"number of spam emails:\", len(allSpamData))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-invite",
   "metadata": {
    "papermill": {
     "duration": 0.016553,
     "end_time": "2021-06-15T14:09:26.756468",
     "exception": false,
     "start_time": "2021-06-15T14:09:26.739915",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NLTK's Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "funded-louisville",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T14:09:26.798370Z",
     "iopub.status.busy": "2021-06-15T14:09:26.797670Z",
     "iopub.status.idle": "2021-06-15T14:09:26.800833Z",
     "shell.execute_reply": "2021-06-15T14:09:26.800295Z"
    },
    "papermill": {
     "duration": 0.027561,
     "end_time": "2021-06-15T14:09:26.800978",
     "exception": false,
     "start_time": "2021-06-15T14:09:26.773417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating a preprocessing function\n",
    "# to tokenize and lemmatize the data using NLTK library\n",
    "def preprocess(data):\n",
    "    # tokenization\n",
    "    tokens = nltk.word_tokenize(data)\n",
    "    tokens = [w.lower() for w in tokens if w.isalpha()]\n",
    "\n",
    "    # finding uncommon words\n",
    "    cnt = Counter(tokens)\n",
    "    uncommons = cnt.most_common()[:-int(len(cnt)*0.1):-1]\n",
    "    \n",
    "    # listing stopwords from NLTK\n",
    "    stops = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    # removing stop words and uncommon words\n",
    "    tokens = [w for w in tokens if (w not in stops and w not in uncommons)]\n",
    "\n",
    "    # lemmatization\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "native-building",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T14:09:26.841535Z",
     "iopub.status.busy": "2021-06-15T14:09:26.840767Z",
     "iopub.status.idle": "2021-06-15T14:12:13.747695Z",
     "shell.execute_reply": "2021-06-15T14:12:13.747045Z"
    },
    "papermill": {
     "duration": 166.929645,
     "end_time": "2021-06-15T14:12:13.747881",
     "exception": false,
     "start_time": "2021-06-15T14:09:26.818236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenize and lemmatize the data\n",
    "allHamDataProcessed = [preprocess(data) for data in allHamData]\n",
    "allSpamDataProcessed = [preprocess(data) for data in allSpamData]\n",
    "\n",
    "# create a feature from this data by combining it, called X\n",
    "X = allHamDataProcessed + allSpamDataProcessed\n",
    "\n",
    "# create a label array called y to differenciate between spam and ham\n",
    "# True will denote it as spam, and False will denote ham\n",
    "y = [False]*len(allHamDataProcessed) + [True]*len(allSpamDataProcessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "extreme-improvement",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T14:12:14.105187Z",
     "iopub.status.busy": "2021-06-15T14:12:14.099016Z",
     "iopub.status.idle": "2021-06-15T14:12:14.135576Z",
     "shell.execute_reply": "2021-06-15T14:12:14.134934Z"
    },
    "papermill": {
     "duration": 0.366492,
     "end_time": "2021-06-15T14:12:14.135715",
     "exception": false,
     "start_time": "2021-06-15T14:12:13.769223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               email  label\n",
      "0  [subject, million, dollar, content, louise, tr...  False\n",
      "1  [subject, mitsubishi, turbine, louise, spoke, ...  False\n",
      "2  [subject, start, date, hourahead, hour, start,...  False\n",
      "3  [subject, enpwer, eol, data, eol, deal, enpowe...  False\n",
      "4  [subject, caiso, notice, upcoming, mif, stakeh...  False\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataframe out of the processed data\n",
    "dataframeProcessed = pd.DataFrame({\"email\": X, \"label\": y})\n",
    "\n",
    "# Get an overview of the processed data\n",
    "print(dataframeProcessed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "going-transparency",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T14:12:14.247168Z",
     "iopub.status.busy": "2021-06-15T14:12:14.209929Z",
     "iopub.status.idle": "2021-06-15T14:12:15.099378Z",
     "shell.execute_reply": "2021-06-15T14:12:15.099949Z"
    },
    "papermill": {
     "duration": 0.945732,
     "end_time": "2021-06-15T14:12:15.100152",
     "exception": false,
     "start_time": "2021-06-15T14:12:14.154420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting the data into (X, y) with train:test = 70:30\n",
    "X, y = dataframeProcessed[\"email\"], dataframeProcessed[\"label\"]\n",
    "\n",
    "X_featurized = [Counter(i) for i in X]\n",
    "allDataProcessed = [(X_featurized[i], y[i]) for i in range(len(X))]\n",
    "shuffle(allDataProcessed)\n",
    "trainData, testData = allDataProcessed[:int(len(allDataProcessed)*0.7)], allDataProcessed[int(len(allDataProcessed)*0.7):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "coordinate-paris",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T14:12:15.181877Z",
     "iopub.status.busy": "2021-06-15T14:12:15.149180Z",
     "iopub.status.idle": "2021-06-15T14:12:51.866878Z",
     "shell.execute_reply": "2021-06-15T14:12:51.867393Z"
    },
    "papermill": {
     "duration": 36.746876,
     "end_time": "2021-06-15T14:12:51.867574",
     "exception": false,
     "start_time": "2021-06-15T14:12:15.120698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with NLTK's Naive Bayes classifier is: 0.9881941407958024\n",
      "Accuracy over the whole dataset is: 0.9920965467484341\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "model_nltkNaiveBayes = nltk.classify.NaiveBayesClassifier.train(trainData)\n",
    "\n",
    "# Testing the model\n",
    "testing_accuracy = nltk.classify.accuracy(model_nltkNaiveBayes, testData)\n",
    "print(\"Accuracy with NLTK's Naive Bayes classifier is:\", testing_accuracy)\n",
    "\n",
    "# Checking accuracy for the whole dataset\n",
    "whole_accuracy = nltk.classify.accuracy(model_nltkNaiveBayes, allDataProcessed)\n",
    "print(\"Accuracy over the whole dataset is:\", whole_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-cradle",
   "metadata": {
    "papermill": {
     "duration": 0.020603,
     "end_time": "2021-06-15T14:12:51.908077",
     "exception": false,
     "start_time": "2021-06-15T14:12:51.887474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Scikit-learn's Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "historical-entertainment",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T14:12:51.952255Z",
     "iopub.status.busy": "2021-06-15T14:12:51.951562Z",
     "iopub.status.idle": "2021-06-15T14:12:51.955415Z",
     "shell.execute_reply": "2021-06-15T14:12:51.954621Z",
     "shell.execute_reply.started": "2021-06-15T13:03:57.802249Z"
    },
    "papermill": {
     "duration": 0.028516,
     "end_time": "2021-06-15T14:12:51.955574",
     "exception": false,
     "start_time": "2021-06-15T14:12:51.927058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating features and labels from raw, unprocessed data\n",
    "X = allHamData + allSpamData\n",
    "y = [0]*len(allHamData) + [1]*len(allSpamData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "elegant-local",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T14:12:52.011343Z",
     "iopub.status.busy": "2021-06-15T14:12:52.005510Z",
     "iopub.status.idle": "2021-06-15T14:12:54.115377Z",
     "shell.execute_reply": "2021-06-15T14:12:54.116334Z",
     "shell.execute_reply.started": "2021-06-15T13:04:47.401942Z"
    },
    "papermill": {
     "duration": 2.141042,
     "end_time": "2021-06-15T14:12:54.116594",
     "exception": false,
     "start_time": "2021-06-15T14:12:51.975552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different words used in the dataset: 159211\n"
     ]
    }
   ],
   "source": [
    "# getting an overview of the number of different words \n",
    "# used all over the dataset\n",
    "\n",
    "all = []\n",
    "for i in X:\n",
    "    all += list(set(i.split()))\n",
    "    \n",
    "all = list(set(all))\n",
    "numDiffWords = len(all)\n",
    "\n",
    "print(\"Number of different words used in the dataset:\", numDiffWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dental-webmaster",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T14:12:54.182209Z",
     "iopub.status.busy": "2021-06-15T14:12:54.169798Z",
     "iopub.status.idle": "2021-06-15T14:13:03.656494Z",
     "shell.execute_reply": "2021-06-15T14:13:03.655563Z",
     "shell.execute_reply.started": "2021-06-15T13:04:57.352129Z"
    },
    "papermill": {
     "duration": 9.51813,
     "end_time": "2021-06-15T14:13:03.656691",
     "exception": false,
     "start_time": "2021-06-15T14:12:54.138561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vectorize the features using CountVectorizer\n",
    "vec = sk.feature_extraction.text.CountVectorizer(max_features = int(numDiffWords*0.3))\n",
    "X_vectorized = vec.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "racial-nightmare",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T14:13:03.702406Z",
     "iopub.status.busy": "2021-06-15T14:13:03.700740Z",
     "iopub.status.idle": "2021-06-15T14:13:03.730755Z",
     "shell.execute_reply": "2021-06-15T14:13:03.730037Z",
     "shell.execute_reply.started": "2021-06-15T13:05:08.381437Z"
    },
    "papermill": {
     "duration": 0.053651,
     "end_time": "2021-06-15T14:13:03.730902",
     "exception": false,
     "start_time": "2021-06-15T14:13:03.677251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting the data into test and train datasets\n",
    "# using sklearn's train_test_split \n",
    "X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X_vectorized, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ultimate-shopping",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T14:13:03.779390Z",
     "iopub.status.busy": "2021-06-15T14:13:03.778633Z",
     "iopub.status.idle": "2021-06-15T14:13:52.984953Z",
     "shell.execute_reply": "2021-06-15T14:13:52.984198Z",
     "shell.execute_reply.started": "2021-06-15T14:04:16.170196Z"
    },
    "papermill": {
     "duration": 49.231496,
     "end_time": "2021-06-15T14:13:52.985170",
     "exception": false,
     "start_time": "2021-06-15T14:13:03.753674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Sklearn's Random Forest Classifier: 0.984696108439003\n",
      "Accuracy over the whole dataset: 0.9954087823434886\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_rfclassifier = RandomForestClassifier(random_state=0)\n",
    "model_rfclassifier.fit(X_train, y_train)\n",
    "\n",
    "# Testing the model\n",
    "y_predicted = model_rfclassifier.predict(X_test)\n",
    "\n",
    "# Checking accuracy\n",
    "accuracy_rfclassifier = sk.metrics.accuracy_score(y_test, y_predicted)\n",
    "print(\"Accuracy with Sklearn's Random Forest Classifier:\", accuracy_rfclassifier)\n",
    "\n",
    "print(\"Accuracy over the whole dataset:\", sk.metrics.accuracy_score(model_rfclassifier.predict(X_vectorized), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-seminar",
   "metadata": {
    "papermill": {
     "duration": 0.020458,
     "end_time": "2021-06-15T14:13:53.025917",
     "exception": false,
     "start_time": "2021-06-15T14:13:53.005459",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Hyperparameters tuning of RandomForestClassifier using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "green-coordinator",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T14:13:53.073077Z",
     "iopub.status.busy": "2021-06-15T14:13:53.072435Z",
     "iopub.status.idle": "2021-06-15T14:13:53.076210Z",
     "shell.execute_reply": "2021-06-15T14:13:53.075359Z",
     "shell.execute_reply.started": "2021-06-15T13:14:36.027530Z"
    },
    "papermill": {
     "duration": 0.030472,
     "end_time": "2021-06-15T14:13:53.076371",
     "exception": false,
     "start_time": "2021-06-15T14:13:53.045899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Declaring different possible values for the parameters\n",
    "n_estimators = [int(x) for x in range(10, 100, 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [2,4]\n",
    "min_samples_split = [2, 5]\n",
    "min_samples_leaf = [1, 2]\n",
    "bootstrap = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "palestinian-happiness",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T14:13:53.123165Z",
     "iopub.status.busy": "2021-06-15T14:13:53.122160Z",
     "iopub.status.idle": "2021-06-15T14:13:53.126405Z",
     "shell.execute_reply": "2021-06-15T14:13:53.126893Z",
     "shell.execute_reply.started": "2021-06-15T13:19:37.003281Z"
    },
    "papermill": {
     "duration": 0.030329,
     "end_time": "2021-06-15T14:13:53.127058",
     "exception": false,
     "start_time": "2021-06-15T14:13:53.096729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grid looks like:\n",
      "\n",
      "n_estimators: [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
      "max_features: ['auto', 'sqrt']\n",
      "max_depth: [2, 4]\n",
      "min_samples_split: [2, 5]\n",
      "min_samples_leaf: [1, 2]\n",
      "bootstrap: [True, False]\n"
     ]
    }
   ],
   "source": [
    "# Create the param grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Getting an overview of the grid\n",
    "print(\"The grid looks like:\\n\")\n",
    "for key, val in param_grid.items():\n",
    "    print(key + \":\", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "romance-tractor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T14:13:53.181610Z",
     "iopub.status.busy": "2021-06-15T14:13:53.180902Z",
     "iopub.status.idle": "2021-06-15T14:17:45.027978Z",
     "shell.execute_reply": "2021-06-15T14:17:45.028501Z",
     "shell.execute_reply.started": "2021-06-15T13:40:29.454629Z"
    },
    "papermill": {
     "duration": 231.881807,
     "end_time": "2021-06-15T14:17:45.028690",
     "exception": false,
     "start_time": "2021-06-15T14:13:53.146883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tuned parameters:\n",
      " {'bootstrap': True, 'max_depth': 4, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 90}\n"
     ]
    }
   ],
   "source": [
    "# Loading the grid with parameters and the model\n",
    "rf_Grid = sk.model_selection.GridSearchCV(estimator = model_rfclassifier, param_grid = param_grid, cv = 3, n_jobs = 4)\n",
    "\n",
    "# Fitting with data\n",
    "rf_Grid.fit(X_train, y_train)\n",
    "\n",
    "# Getting the tuned parameters\n",
    "print(\"The tuned parameters:\\n\", rf_Grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "major-weekly",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T14:17:45.080247Z",
     "iopub.status.busy": "2021-06-15T14:17:45.079339Z",
     "iopub.status.idle": "2021-06-15T14:17:47.764622Z",
     "shell.execute_reply": "2021-06-15T14:17:47.764072Z",
     "shell.execute_reply.started": "2021-06-15T14:05:04.669180Z"
    },
    "papermill": {
     "duration": 2.714215,
     "end_time": "2021-06-15T14:17:47.764776",
     "exception": false,
     "start_time": "2021-06-15T14:17:45.050561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with hyperparameters tuned Random Forest Classifier: 0.9209663314385658\n",
      "Accuracy over the whole dataset, with tuned model: 0.9223756271931263\n"
     ]
    }
   ],
   "source": [
    "# Using the tuned parameters on the model\n",
    "model_tuned_rfclassifier = RandomForestClassifier(random_state=0, bootstrap= False, max_depth=4, max_features='auto', min_samples_leaf= 1, min_samples_split= 5, n_estimators= 90)\n",
    "\n",
    "# Training the model\n",
    "model_tuned_rfclassifier.fit(X_train, y_train)\n",
    "\n",
    "# Testing the model\n",
    "y_predicted = model_tuned_rfclassifier.predict(X_test)\n",
    "\n",
    "# Checking accuracy\n",
    "accuracy_tuned_rfclassifier = sk.metrics.accuracy_score(y_test, y_predicted)\n",
    "print(\"Accuracy with hyperparameters tuned Random Forest Classifier:\", accuracy_tuned_rfclassifier)\n",
    "\n",
    "print(\"Accuracy over the whole dataset, with tuned model:\", sk.metrics.accuracy_score(model_tuned_rfclassifier.predict(X_vectorized), y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 712.206965,
   "end_time": "2021-06-15T14:17:50.172539",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-15T14:05:57.965574",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
